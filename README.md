# Some key notes on Methods and data collection from the paper

### Methods: 

* probabilistic topic assignment vector was constructed using LDA 
* sentence embeddings were generated using BERT
*  the probabilstic topic assignment vector and the Bert embeddings of the sentences were combined togother in 
   order to balance the information contenct of each vector
*  lower-dimensional latent space representation of the combined vector was generated
*  The Silhouette Score was calculated in order to measure the clusters quality (the nearer the value to one,the
   better the quality)
   
   
   
### Data collection: 
*  Inside the corpus , the following keywords inside the title , keyword , and abstract were searched: like 
   "artifical intelligence" OR "AI" AND "sustainable" OR "Sustainability" And "energy"
*  This resulted in the retrievel of 981 documents

*  The document type was restricted to article and the langauge to "English" and this exclusion resulted in 296 
   articles. 

*  Following that , the titles and abstracts of the articles were manually evaluated to identify the most pertinent    ones that examined the role of artifical intelligence in ensuring the energy sector's sustainability and this  
   screnning yield 182 publications spanning the years 2004 and 2022. 

 

### Repository structure

``` plain
‚îú‚îÄ‚îÄ üìÅ data                 <-- Data sources (stored in the following order):
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data             <-- Raw data extracted from websites
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processed_data   <-- Preprocessed data
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ filtered         <-- Filtered data: abstracts matching query (AI/sustainable/energy)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ concatenated     <-- Concatenated filtered data from three sources for topic modeling
‚îÇ
‚îú‚îÄ‚îÄ üìÅ results              <-- Visualizations of topic modeling results
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks            <-- Jupyter notebook files
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src                  <-- Project code base (executed in the following order):
‚îÇ    ‚îú‚îÄ‚îÄ data_extraction     <-- Data extraction, query selection, and deduplication
‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ scrapers       <-- Web scrapers for abstract extraction
‚îÇ    ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ acm_links_scraper.py       <-- ACM website links scraper
‚îÇ    ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ acm_abstract_scraper.py    <-- ACM website abstract scraper
‚îÇ    ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ science_direct_data_scraper.py <-- Science Direct website scraper
‚îÇ    ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ xplore.py  <-- Xplore website scraper
‚îÇ    ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ xplore_enrich.py <-- Xplore website scraper (enriched)
‚îÇ    ‚îÇ    ‚îÇ
‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ query_selection <-- Query selection logic
‚îÇ    ‚îÇ    ‚îÇ
‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ deduplicating   <-- Deduplication of combined data
‚îÇ    ‚îÇ        ‚îî‚îÄ‚îÄ deduplicating.py <-- Duplicated papers removal
‚îÇ    ‚îÇ        ‚îî‚îÄ‚îÄ filter_out_html_tags.py <-- Removal of HTML tags
‚îÇ    ‚îÇ
‚îÇ    ‚îú‚îÄ‚îÄ data_preprocessing  <-- Preprocessing of scraped data
‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py 
‚îÇ    ‚îÇ
‚îÇ    ‚îú‚îÄ‚îÄ nlp_analytics       <-- Topic modeling implementation
‚îÇ
‚îú‚îÄ‚îÄ üìÉ .gitignore           <-- Git ignore list
‚îÇ
‚îú‚îÄ‚îÄ üìÉ environment.yml      <-- Conda environment dependencies
‚îÇ
‚îú‚îÄ‚îÄ üìÉ main.py              <-- Algorithm comparison
‚îÇ
‚îú‚îÄ‚îÄ üìÉ pyproject.toml       <-- Configuration file
‚îÇ
‚îî‚îÄ‚îÄ üìÉ README.md            <-- Project documentation

```








































